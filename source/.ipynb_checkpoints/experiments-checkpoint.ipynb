{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 15:17:16.733732: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-26 15:17:16.733754: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "import sklearn.metrics as metrics\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed before each experiment -> reproducible results\n",
    "def reset_random():\n",
    "    seed = 7655\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    rng = np.random.default_rng()\n",
    "reset_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(run):\n",
    "    print(\"Load data\")\n",
    "    path_dictionary = {\n",
    "        \"cropped_oldLabelling\" : \"../data/train/cropped_oldLabelling/train_set_128px.pkl\",\n",
    "        \"uncropped_oldLabelling\" : \"../data/train/uncropped_oldLabelling/train_set_128px.pkl\"\n",
    "    }\n",
    "    \n",
    "    data = pd.read_pickle(path_dictionary[run[\"dataset\"]])\n",
    "    # use k fold cross validation later :)\n",
    "    #data = data.loc[data['label_habit'] == \"Column\"]\n",
    "    print(len(data))\n",
    "    train_data, val_data = train_test_split(data, stratify=data[\"label_proc_rimed\"], test_size=0.25, random_state=7655)\n",
    "    del data\n",
    "    #print(val_data.head(2))\n",
    "    return train_data, val_data\n",
    "    #return train_data[:100], val_data[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_test(run):\n",
    "    print(\"Load data test\")\n",
    "    path_dictionary = {\n",
    "        \"cropped_oldLabelling\" : \"../data/test/cropped_oldLabelling/test_set_128px.pkl\",\n",
    "        \"uncropped_oldLabelling\" : \"../data/test/uncropped_oldLabelling/test_set_128px.pkl\"\n",
    "    }    \n",
    "    data = pd.read_pickle(path_dictionary[run[\"dataset\"]])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sample(X_train, Y_train):\n",
    "    n_tot = len(Y_train)\n",
    "    n_pos = np.sum(Y_train)\n",
    "    n_neg = n_tot - n_pos\n",
    "    print(n_tot, n_pos, n_neg)\n",
    "    print(n_neg/(n_neg-n_pos))\n",
    "    \n",
    "    # Undersample Data -> Balance it\n",
    "    neg_idx = np.where(Y_train==0)[0]\n",
    "    #print(neg_idx.shape)\n",
    "    print(neg_idx)\n",
    "    print(type(neg_idx))\n",
    "    idx_del = np.random.choice(n_neg, size=n_neg-n_pos, replace=False)\n",
    "    Y_train = np.delete(Y_train, neg_idx[idx_del])\n",
    "    X_train = np.delete(X_train, neg_idx[idx_del], axis=0)\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_sample(X_train, Y_train):\n",
    "    n_tot = len(Y_train)\n",
    "    n_pos = np.sum(Y_train)\n",
    "    n_neg = n_tot - n_pos\n",
    "    print(n_tot, n_pos, n_neg)\n",
    "    #print(n_neg/(n_neg-n_pos))\n",
    "    \n",
    "    if n_pos == 0:\n",
    "        print(\"Warning!!!!, upsampling with no positive samples! (func: up_sample)\")\n",
    "        return X_train, Y_train\n",
    "    # Oversample Data -> Balance it\n",
    "    times_whole_positive = n_neg // n_pos\n",
    "    extra_positive = n_neg % n_pos\n",
    "    print(times_whole_positive, extra_positive)\n",
    "        \n",
    "    pos_idx = np.where(Y_train==1)[0]\n",
    "    Y_train_pos = Y_train[pos_idx]\n",
    "    X_train_pos = X_train[pos_idx]\n",
    "    \n",
    "    for i in range(times_whole_positive-1):\n",
    "        Y_train = np.concatenate((Y_train, Y_train_pos), axis=0)\n",
    "        X_train = np.concatenate((X_train, X_train_pos), axis=0)\n",
    "        \n",
    "    Y_train = np.concatenate((Y_train, Y_train_pos[:extra_positive]), axis=0)\n",
    "    X_train = np.concatenate((X_train, X_train_pos[:extra_positive]), axis=0)\n",
    "    \n",
    "    del Y_train_pos, X_train_pos\n",
    "    \n",
    "    assert len(Y_train) == len(X_train)\n",
    "    p = np.random.permutation(len(Y_train))\n",
    "    Y_train = Y_train[p]\n",
    "    X_train = X_train[p]\n",
    "    \n",
    "    n_tot = len(Y_train)\n",
    "    n_pos = np.sum(Y_train)\n",
    "    n_neg = n_tot - n_pos\n",
    "    print(n_tot, n_pos, n_neg)\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_sample_augment(X_train, Y_train):\n",
    "    # total length is going to be 8*len(smaller class) (8=4*rotation,flip, 4*rotation again)\n",
    "    n_tot = len(Y_train)\n",
    "    n_pos = np.sum(Y_train)\n",
    "    n_neg = n_tot - n_pos\n",
    "    print(n_tot, n_pos, n_neg)\n",
    "    # 1356 117 1239\n",
    "    #print(n_neg/(n_neg-n_pos))\n",
    "        \n",
    "    pos_idx = np.where(Y_train==1)[0]\n",
    "    Y_train_pos = Y_train[pos_idx]\n",
    "    X_train_pos = X_train[pos_idx]\n",
    "    \n",
    "    neg_idx = np.where(Y_train==0)[0]\n",
    "    Y_train_neg = Y_train[neg_idx]\n",
    "    X_train_neg = X_train[neg_idx]\n",
    "\n",
    "    del X_train, Y_train\n",
    "    \n",
    "    X_train_pos, Y_train_pos = rotate_and_flip(X_train_pos, Y_train_pos)\n",
    "    X_train_neg, Y_train_neg = rotate_and_flip(X_train_neg, Y_train_neg, n=n_pos*8)\n",
    "    \n",
    "    Y_train = np.concatenate((Y_train_neg, Y_train_pos), axis=0)\n",
    "    X_train = np.concatenate((X_train_neg, X_train_pos), axis=0)\n",
    "    del Y_train_neg, Y_train_pos, X_train_neg, X_train_pos\n",
    "\n",
    "    assert len(Y_train) == len(X_train)\n",
    "    p = np.random.permutation(len(Y_train))\n",
    "    Y_train = Y_train[p]\n",
    "    X_train = X_train[p]\n",
    "    \n",
    "    n_tot = len(Y_train)\n",
    "    n_pos = np.sum(Y_train)\n",
    "    n_neg = n_tot - n_pos\n",
    "    print(n_tot, n_pos, n_neg)\n",
    "    return X_train, Y_train\n",
    "# 1872 27862362.0 -27860490.0\n",
    "\n",
    "def rotate_and_flip(X_train, Y_train, n=0):\n",
    "    print(\"rotate and flip\")\n",
    "    X_train = np.concatenate((X_train, np.rot90(X_train, axes=(1, 2))))\n",
    "    X_train = np.concatenate((X_train, np.rot90(X_train, axes=(1, 2), k=2)))\n",
    "    #X_train = np.concatenate((X_train, np.flip(X_train)))\n",
    "    X_train = np.concatenate((X_train, np.flipud(X_train)))\n",
    "    \n",
    "    Y_train = np.tile(Y_train,8)\n",
    "    \n",
    "    if n > 0:\n",
    "        # random subsample to n\n",
    "        n_tot = len(Y_train)\n",
    "        idx_del = np.random.choice(n_tot, size=n_tot-n, replace=False)\n",
    "        Y_train = np.delete(Y_train, idx_del)\n",
    "        X_train = np.delete(X_train, idx_del, axis=0)\n",
    "        \n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to mean = 0, std = 1\n",
    "def standardize(images):\n",
    "    mean = images.mean(axis=(1,2), keepdims=True)\n",
    "    std = images.std(axis=(1,2), keepdims=True)\n",
    "    images = (images - mean) / std\n",
    "    return images\n",
    "\n",
    "# transform into range [0,1]\n",
    "def normalize1(images):\n",
    "    minimum = np.min(images, axis=(1,2), keepdims=True)\n",
    "    maximum = np.max(images, axis=(1,2), keepdims=True)\n",
    "    return (images - minimum) / (maximum-minimum)\n",
    "\n",
    "# transform into range [-1,1]\n",
    "def normalize2(images):\n",
    "    return (2*normalize1(images))-1\n",
    "\n",
    "# transform like in https://towardsdatascience.com/data-preprocessing-and-network-building-in-cnn-15624ef3a28b -> Normalization\n",
    "def normalize3(images):\n",
    "    minimum = np.min(images, axis=(1,2), keepdims=True)\n",
    "    maximum = np.max(images, axis=(1,2), keepdims=True)\n",
    "    return images - (minimum / maximum) - minimum\n",
    "\n",
    "def centering(images):\n",
    "    mean = np.mean(images, axis=(1,2), keepdims=True)\n",
    "    return normalize1(images-mean)\n",
    "\n",
    "def normalize_and_standardize(images):\n",
    "    images = normalize2(images)\n",
    "    mean = np.mean(images)\n",
    "    std = np.std(images)\n",
    "    images = (images - mean) / std\n",
    "    return images\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(data, ds_type, run):\n",
    "    print(\"Preprocess data for 3 channels\", end=\" \")\n",
    "    \n",
    "    X_abs = run[\"normalize\"](np.stack(data[\"img_abs\"]))\n",
    "    X_ang = run[\"normalize\"](np.stack(data[\"img_ang\"]))\n",
    "    X = np.stack((X_abs, X_abs, X_ang), axis=-1)\n",
    "    Y = data[run[\"label\"]].to_numpy()\n",
    "    del X_abs, X_ang, data\n",
    "    \n",
    "    n_tot = len(Y)\n",
    "    n_pos = np.sum(Y)\n",
    "    n_neg = n_tot - n_pos\n",
    "    print(n_pos / n_tot, n_neg / n_tot)\n",
    "    \n",
    "    if ds_type == \"train\":\n",
    "        if run[\"balance_dataset\"] == \"down_sampling\":\n",
    "            X_train, Y_train = down_sample(X_train, Y_train)    \n",
    "        elif run[\"balance_dataset\"] == \"up_sampling\":\n",
    "            X_train, Y_train = up_sample(X_train, Y_train)\n",
    "        elif run[\"balance_dataset\"] == \"augment\":\n",
    "            X_train, Y_train = up_sample_augment(X_train, Y_train)\n",
    "    \n",
    "        batches = (\n",
    "            tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "            .cache()\n",
    "            .shuffle(run[\"BUFFER_SIZE\"])\n",
    "            .batch(run[\"BATCH_SIZE\"])\n",
    "            .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )\n",
    "    else:\n",
    "        batches = (\n",
    "            tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "            .cache()\n",
    "            .batch(run[\"BATCH_SIZE\"])\n",
    "            .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )\n",
    "    \n",
    "    del X\n",
    "    return batches, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelDenseNet121(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "    base_model = tf.keras.applications.densenet.DenseNet121(\n",
    "        include_top=False, weights=pretrained_weights,\n",
    "        input_shape=IMG_SHAPE, pooling='max')\n",
    "    \n",
    "    if run[\"pretrained\"]:\n",
    "        base_model.trainable = False\n",
    "    \n",
    "    #base_model.summary()\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    x = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\")(inputs)\n",
    "    x = base_model(x, training=False)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model, base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelDenseNet121_noHead(run):\n",
    "    pr\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "    base_model = tf.keras.applications.densenet.DenseNet121(\n",
    "        include_top=False, weights=pretrained_weights,\n",
    "        input_shape=IMG_SHAPE, pooling='max')\n",
    "    \n",
    "    base_model.trainable = True\n",
    "    \n",
    "    #base_model.summary()\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    x = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\")(inputs)\n",
    "    x = base_model(x, training=True)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model, base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelDenseNet201(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "    base_model = tf.keras.applications.densenet.DenseNet201(\n",
    "        include_top=False, weights=pretrained_weights,\n",
    "        input_shape=IMG_SHAPE, pooling='max')\n",
    "    \n",
    "    if run[\"pretrained\"]:\n",
    "        base_model.trainable = False\n",
    "    \n",
    "    #base_model.summary()\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    #x = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\")(inputs)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-center each color channel with respect to the ImageNet dataset, without scaling.\n",
    "def getModelResNet50(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        include_top=False, weights=pretrained_weights,\n",
    "        input_shape=IMG_SHAPE, pooling='max')\n",
    "    \n",
    "    if run[\"pretrained\"]:\n",
    "        base_model.trainable = False\n",
    "    \n",
    "    #base_model.summary()\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    #x = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\")(inputs)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-center each color channel with respect to the ImageNet dataset, without scaling.\n",
    "def getModelResNet152(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "    base_model = tf.keras.applications.ResNet152(\n",
    "        include_top=False, weights=pretrained_weights,\n",
    "        input_shape=IMG_SHAPE, pooling='max')\n",
    "    \n",
    "    if run[\"pretrained\"]:\n",
    "        base_model.trainable = False\n",
    "    \n",
    "    #base_model.summary()\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    #x = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\")(inputs)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelMobileNetV2(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights=pretrained_weights, pooling='max')\n",
    "    if run[\"pretrained\"]:\n",
    "        base_model.trainable = False\n",
    "    #base_model.summary()\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelMobileNetV2_nohead(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights=pretrained_weights, pooling='max')\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    x = base_model(inputs, training=True)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(run):\n",
    "    initial_bias = run[\"initial_bias\"]\n",
    "    if initial_bias:\n",
    "        initial_bias = np.log([642/3551])\n",
    "    print(\"Make model\",initial_bias)\n",
    "\n",
    "    model, base_model = run[\"model\"](run)\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(optimizer = run[\"optimizer\"](learning_rate=run[\"learning_rate\"]),\n",
    "                  loss = run[\"loss\"],\n",
    "                  metrics = ['binary_accuracy',\n",
    "                             'hinge',\n",
    "                             tf.keras.metrics.AUC(name='auc'),\n",
    "                             tf.keras.metrics.Recall(name='recall'),\n",
    "                             tf.keras.metrics.Precision(name='precision')]\n",
    "             )\n",
    "    return model, base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEarlyStopping(keras.callbacks.Callback):\n",
    "    def __init__(self, patience=0):\n",
    "        super(CustomEarlyStopping, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best_f1 = 0.0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "        recall=logs.get('val_recall')\n",
    "        precision=logs.get('val_precision')\n",
    "        f1 = 2*recall*precision/(recall+precision)\n",
    "\n",
    "        if np.greater(f1, self.best_f1):\n",
    "            self.best_f1 = f1\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (greater).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(image, label):\n",
    "    #true_frac = 0.14963822851236383\n",
    "    #weights = tf.constant([true_frac, 1.0 - true_frac])\n",
    "    #weights = tf.constant([5.68 / 6.68, 1.0 / 6.68]) \n",
    "    weights = tf.constant([0.16299749633082966, 0.83700250366917])\n",
    "    sample_weights = tf.gather(weights, indices=tf.cast(label, tf.int64))\n",
    "    return image, label, sample_weights\n",
    "\n",
    "def train_model(model, train_batches, val_batches, run):\n",
    "    print(\"Train\")\n",
    "    #if run[\"weight\"]:\n",
    "    #    history = model.fit(train_batches.map(weights), epochs=run[\"epochs\"], \n",
    "    #                validation_data=val_batches, verbose=run[\"verbose\"])\n",
    "    #else:\n",
    "    \n",
    "    if run[\"early_stopping\"]:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=3, verbose=1,restore_best_weights=True)\n",
    "        history = model.fit(train_batches, epochs=run[\"epochs\"], \n",
    "                    validation_data=val_batches, verbose=run[\"verbose\"], callbacks=[CustomEarlyStopping(patience=15)])\n",
    "    else:\n",
    "        history = model.fit(train_batches, epochs=run[\"epochs\"], \n",
    "                        validation_data=val_batches, verbose=run[\"verbose\"])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history,Y_pred_prob, Y_val):\n",
    "    plt.plot(history.history['binary_accuracy'])\n",
    "    plt.plot(history.history['val_binary_accuracy'])\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.plot(history.history['hinge'])\n",
    "    plt.plot(history.history['val_hinge'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['accuracy', 'val_accuracy', 'loss', 'val_loss', 'hinge', 'val_hinge'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raw_metrics(history,Y_pred_prob, Y_val):\n",
    "    prec = np.array(history.history['precision'])\n",
    "    rec = np.array(history.history['recall'])\n",
    "    val_prec = np.array(history.history['val_precision'])\n",
    "    val_rec = np.array(history.history['val_recall'])\n",
    "\n",
    "    # avoid division by zero\n",
    "    tol = 10e-7   \n",
    "    summe = prec + rec\n",
    "    val_summe = val_prec + val_rec\n",
    "    \n",
    "    summe = np.where(summe == 0.0, tol, summe)\n",
    "    val_summe = np.where(val_summe == 0.0, tol, val_summe)\n",
    "    \n",
    "    f1 = 2 * (prec * rec) / summe\n",
    "    val_f1 = 2 * (val_prec * val_rec) / val_summe\n",
    "    \n",
    "    plt.plot(history.history['recall'])\n",
    "    plt.plot(history.history['val_recall'])\n",
    "    plt.plot(history.history['precision'])\n",
    "    plt.plot(history.history['val_precision'])\n",
    "    plt.plot(f1)\n",
    "    plt.plot(val_f1)\n",
    " \n",
    "    plt.title('Recall & Precision')\n",
    "    plt.ylabel('rate')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['recall', 'val_recall', 'precision', 'val_precision', 'f1', 'val_f1'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC(history,Y_pred_prob, Y_val):\n",
    "    fpr, tpr, threshold = metrics.roc_curve(Y_val, Y_pred_prob)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    print(\"AUC = \"+ str(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(history,Y_pred_prob, Y_val):\n",
    "    plot_metrics(history,Y_pred_prob, Y_val)\n",
    "    plot_raw_metrics(history,Y_pred_prob, Y_val)\n",
    "    plot_ROC(history,Y_pred_prob, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, val_batches, run):\n",
    "    print(\"Make predictions\")\n",
    "    Y_pred_prob = model.predict(val_batches)\n",
    "    print(Y_pred_prob.shape)\n",
    "    print(Y_pred_prob)\n",
    "    print(np.sum(Y_pred_prob))\n",
    "    # TODO: change np where !\n",
    "    #Y_pred = np.argmax(Y_pred_prob, axis=1)\n",
    "    print(\"YPredProb \"+str(Y_pred_prob.shape))\n",
    "    print(Y_pred_prob)\n",
    "    Y_pred = np.where(Y_pred_prob < 0.5, 0, 1)[:,0]\n",
    "    print(\"###############\", Y_pred.shape)\n",
    "    print(Y_pred)\n",
    "    print(np.sum(Y_pred))\n",
    "    return Y_pred, Y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluate and save evaluation and run in a logfile\n",
    "def evaluate_model(y_true, y_pred, y_pred_prob, history, val_batches, run):\n",
    "    #model_summary = history.model.summary()\n",
    "    params = history.params\n",
    "    datestr = time.strftime(\"%y:%m:%d\")\n",
    "    timestr = time.strftime(\"%H:%M:%S\")\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    conf_mat_percent = 100 * conf_mat / len(y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bce = log_loss(y_true, y_pred)\n",
    "    print(run[\"epochs\"])\n",
    "    print(conf_mat)\n",
    "    print(conf_mat_percent)\n",
    "    print(\"TN: \"+str(conf_mat[0,0]))\n",
    "    print(\"FP: \"+str(conf_mat[0,1]))\n",
    "    print(\"FN: \"+str(conf_mat[1,0]))\n",
    "    print(\"TP: \"+str(conf_mat[1,1]))\n",
    "    print(\"F1: \",f1)\n",
    "    print(\"Accuracy: \",acc)\n",
    "    print(\"Binary Cross Entropy: \", bce)\n",
    "    \n",
    "    if run[\"save\"]:       \n",
    "        filename = \"../logs/\"+datestr+\"/\"+timestr+\"_\"+run[\"label\"]+\"_\"+str(run[\"epochs\"])+\".txt\"\n",
    "        # make folder if it doesn't exist already\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        \n",
    "        f = open(filename, \"x\")\n",
    "        # write run specification\n",
    "        f.write(\"Label: \"+run[\"label\"]+\"\\n\")\n",
    "        f.write(\"Normalization: \"+run[\"normalize\"].__name__+\"\\n\")\n",
    "        f.write(\"Batch Size: \"+str(run[\"BATCH_SIZE\"])+\"\\n\")\n",
    "        f.write(\"Buffer Size: \"+str(run[\"BUFFER_SIZE\"])+\"\\n\")\n",
    "        f.write(\"Model Name: \"+run[\"model\"].__name__+\"\\n\")\n",
    "        f.write(\"Optimizer: \"+str(run[\"optimizer\"])+\"\\n\")\n",
    "        f.write(\"Loss: \"+str(run[\"loss\"])+\"\\n\")\n",
    "        f.write(\"Epochs: \"+str(run[\"epochs\"])+\"\\n\")\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        # write evaluation\n",
    "        f.write(\"F1: \"+str(f1)+\"\\n\")\n",
    "        f.write(\"Accuracy: \"+str(acc)+\"\\n\")\n",
    "        f.write(\"Logloss: \"+str(bce)+\"\\n\\n\")\n",
    "        \n",
    "        f.write(str(conf_mat)+\"\\n\\n\")\n",
    "        f.write(str(conf_mat_percent)+\"\\n\\n\")\n",
    "        \n",
    "        # write model summary\n",
    "        f.write(str(history.params) + \"\\n\\n\")\n",
    "        #f.write(str(model_summary))\n",
    "        history.model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "        f.close()\n",
    "        \n",
    "        # Get the dictionary containing each metric and the loss for each epoch\n",
    "        history_filename = \"../logs/history/\"+datestr+\"/\"+timestr+\"_\"+run[\"label\"]+\"_\"+str(run[\"epochs\"])+\".txt\"\n",
    "        os.makedirs(os.path.dirname(history_filename), exist_ok=True)\n",
    "        history_dict = history.history\n",
    "        json.dump(history_dict, open(history_filename, 'w'))\n",
    "    \n",
    "    find_misclassified(y_true, y_pred, y_pred_prob, val_batches, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_bias(y):\n",
    "    pos = np.sum(y)\n",
    "    neg = np.sum(1-y)\n",
    "    initial_bias = np.log([pos/neg])\n",
    "    return initial_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(run, model):\n",
    "    reset_random()\n",
    "    \n",
    "    test_data = load_data_test(run)\n",
    "    \n",
    "    X, Y = preprocess_data(test_data, \"test\", run)\n",
    "    prediction_prob = model.predict(X)\n",
    "    prediction_bool = np.where(prediction_prob < 0.5, 0, 1)[:,0]\n",
    "    \n",
    "    conf_mat = confusion_matrix(Y, prediction_bool)\n",
    "    conf_mat_percent = 100 * conf_mat / len(prediction_bool)\n",
    "    print(conf_mat)\n",
    "    print(conf_mat_percent)\n",
    "    \n",
    "    print(\"Acc: \"+str(accuracy_score(Y, prediction_bool)))\n",
    "    print(\"Bal.Acc: \"+str(balanced_accuracy_score(Y, prediction_bool)))\n",
    "\n",
    "    print(\"TN: \"+str(conf_mat[0,0]))\n",
    "    print(\"FP: \"+str(conf_mat[0,1]))\n",
    "    print(\"FN: \"+str(conf_mat[1,0]))\n",
    "    print(\"TP: \"+str(conf_mat[1,1]))\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bce = log_loss(y_true, y_pred)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    roc_auc_score(Y, )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(run, continue_training=False):\n",
    "    reset_random()\n",
    "    if run[\"verbose\"]:\n",
    "        print(run)\n",
    "        \n",
    "    # start new model\n",
    "    if not continue_training:\n",
    "        train_data, val_data = load_data(run)    \n",
    "        train_batches,  Y_train  = run[\"preprocessing\"](train_data, \"train\", run)\n",
    "        val_batches, Y_val = run[\"preprocessing\"](val_data, \"val\", run)\n",
    "        del train_data, val_data\n",
    "        if run[\"initial_bias\"]:\n",
    "            run[\"initial_bias\"] = initial_bias(Y_train)\n",
    "        del Y_train\n",
    "        model, base_model = make_model(run)\n",
    "        epochs_already_done = 0\n",
    "    \n",
    "    #continue training    \n",
    "    else:\n",
    "        run[\"initial_bias\"] = False\n",
    "        train_batches, val_batches, Y_val, model, old_history = continue_training\n",
    "        epochs_already_done = old_history.params[\"epochs\"]\n",
    "    \n",
    "    history = train_model(model, train_batches, val_batches, run)\n",
    "    #for i in range(9):\n",
    "    #train_model(model, train_batches[:], val_batches, run)   \n",
    "    \n",
    "    #history = model.history\n",
    "    Y_pred, Y_pred_prob = predict(model, val_batches, run)\n",
    "    run[\"plot\"](history,Y_pred_prob, Y_val)\n",
    "    \n",
    "    run[\"epochs\"] += epochs_already_done\n",
    "    evaluate_model(Y_val, Y_pred, Y_pred_prob, history, val_batches, run)\n",
    "    return train_batches, val_batches, Y_val, model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_finetune(run, continue_training=False):\n",
    "    reset_random()\n",
    "    if run[\"verbose\"]:\n",
    "        print(run)\n",
    "        \n",
    "    # start new model\n",
    "    if not continue_training:\n",
    "        train_data, val_data = load_data(run)    \n",
    "        train_batches,  Y_train  = run[\"preprocessing\"](train_data, \"train\", run)\n",
    "        val_batches, Y_val = run[\"preprocessing\"](val_data, \"val\", run)\n",
    "        del train_data, val_data\n",
    "        if run[\"initial_bias\"]:\n",
    "            run[\"initial_bias\"] = initial_bias(Y_train)\n",
    "        del Y_train\n",
    "        model, base_model = make_model(run)\n",
    "        epochs_already_done = 0\n",
    "    \n",
    "    #continue training    \n",
    "    else:\n",
    "        run[\"initial_bias\"] = False\n",
    "        train_batches, val_batches, Y_val, model, old_history = continue_training\n",
    "        epochs_already_done = old_history.params[\"epochs\"]\n",
    "        \n",
    "    history = train_model(model, train_batches, val_batches, run)    \n",
    "    Y_pred, Y_pred_prob = predict(model, val_batches, run)\n",
    "    run[\"plot\"](history,Y_pred_prob, Y_val)\n",
    "    \n",
    "    run[\"epochs\"] += epochs_already_done\n",
    "    evaluate_model(Y_val, Y_pred, Y_pred_prob, history, val_batches, run)\n",
    "    \n",
    "    base_model.trainable = True\n",
    "    # Let's take a look to see how many layers are in the base model\n",
    "    print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_at = 100\n",
    "\n",
    "    # Freeze all the layers before the `fine_tune_at` layer\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model.compile(optimizer = run[\"optimizer\"](learning_rate=run[\"learning_rate\"]/10),\n",
    "                  loss = run[\"loss\"],\n",
    "                  metrics = ['binary_accuracy',\n",
    "                             'hinge',\n",
    "                             tf.keras.metrics.AUC(name='auc'),\n",
    "                             tf.keras.metrics.Recall(name='recall'),\n",
    "                             tf.keras.metrics.Precision(name='precision')]\n",
    "             )\n",
    "    model.summary()\n",
    "    \n",
    "    fine_tune_epochs = 50\n",
    "    total_epochs = fine_tune_epochs + run[\"epochs\"]\n",
    "    \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=3, verbose=1,restore_best_weights=False)\n",
    "    history_fine = model.fit(train_batches, epochs=total_epochs, \n",
    "                    validation_data=val_batches, verbose=run[\"verbose\"], callbacks=[callback],\n",
    "                            initial_epoch=history.epoch[-1])\n",
    "    \n",
    "    Y_pred, Y_pred_prob = predict(model, val_batches, run)\n",
    "    run[\"plot\"](history,Y_pred_prob, Y_val)\n",
    "    \n",
    "    run[\"epochs\"] += epochs_already_done\n",
    "    evaluate_model(Y_val, Y_pred, Y_pred_prob, history, run)\n",
    "    \n",
    "    return train_batches, val_batches, Y_val, model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_misclassified(y_true, y_pred, y_pred_prob, val_batches, run):\n",
    "    print(y_true.shape, y_pred.shape)\n",
    "    wrong = np.square(y_true-y_pred) # 0 if true, 1 if false\n",
    "    indices_wrong = np.where(wrong == 1)\n",
    "    indices_correct = np.where(wrong == 0)\n",
    "    \n",
    "    _, val_data = load_data(run)    \n",
    "    \n",
    "    basic_habit_dict_all = { \n",
    "        \"Column\": 0,\n",
    "        \"Plate\": 0,\n",
    "        \"Droplet\": 0,\n",
    "        \"Lollipop\": 0,\n",
    "        \"Irregular\": 0,\n",
    "        \"Small\": 0,\n",
    "        \"Plate Column\": 0\n",
    "    }    \n",
    "        \n",
    "    for basic_habit in val_data[\"label_habit\"]:\n",
    "        basic_habit_dict_all[basic_habit] += 1\n",
    "    \n",
    "    array_all = np.array(list(basic_habit_dict_all.values()),dtype=np.float32)\n",
    "        \n",
    "    wrong_data = val_data.iloc[indices_wrong]    \n",
    "    basic_habit_dict_wrong = { \n",
    "        \"Column\": 0,\n",
    "        \"Plate\": 0,\n",
    "        \"Droplet\": 0,\n",
    "        \"Lollipop\": 0,\n",
    "        \"Irregular\": 0,\n",
    "        \"Small\": 0,\n",
    "        \"Plate Column\": 0\n",
    "    }\n",
    "    \n",
    "    for basic_habit in wrong_data[\"label_habit\"]:\n",
    "        basic_habit_dict_wrong[basic_habit] += 1\n",
    "        \n",
    "        \n",
    "    basic_habit_dict_percent = {}\n",
    "    for key in basic_habit_dict_all.keys():\n",
    "        if basic_habit_dict_all[key] != 0:\n",
    "            basic_habit_dict_percent[key] = round(100 * float(basic_habit_dict_wrong[key]) / float(basic_habit_dict_all[key]),2)\n",
    "        else:\n",
    "            basic_habit_dict_percent[key] = 0.0\n",
    "        \n",
    "    array_wrong = np.array(list(basic_habit_dict_wrong.values()),dtype=np.float32)\n",
    "   \n",
    "    correct_data = val_data.iloc[indices_correct] \n",
    "    \n",
    "    print(\"All: \",  basic_habit_dict_all)\n",
    "    print(\"Wrong: \",  basic_habit_dict_wrong)\n",
    "    # avoid division by zero!\n",
    "    print(\"Wrong in %: \", basic_habit_dict_percent)\n",
    "    \n",
    "    mean_all = val_data[\"size\"].mean()\n",
    "    median_all = val_data[\"size\"].median()\n",
    "    mean_wrong = wrong_data[\"size\"].mean()\n",
    "    median_wrong = wrong_data[\"size\"].median()\n",
    "    print(\"Mean all: \", mean_all, \" Median all: \", median_all)\n",
    "    print(\"Mean wrong: \", mean_wrong, \" Median wrong: \", median_wrong)\n",
    "    \n",
    "    \n",
    "    bins = np.linspace(0, 300, 50)\n",
    "    plt.hist(wrong_data[\"size\"], bins, alpha=0.5, label='wrong')\n",
    "    plt.hist(correct_data[\"size\"], bins, alpha=0.5, label='correct')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    val_batches_label = np.concatenate([label for image, label in val_batches], axis=0)\n",
    "    val_batches_images = np.concatenate([image for image, label in val_batches], axis=0)\n",
    "\n",
    "\n",
    "    #zipped = list(zip(val_batches_images, val_batches_label, y_pred, y_pred_prob, val_data[\"label_habit\"]))\n",
    "    zipped = list(zip(val_data[\"img_abs\"], val_data[\"img_ang\"], val_batches_label, y_pred, y_pred_prob, val_data[\"label_habit\"], val_data.index.values))\n",
    "    zipped_wrong = [ zipped[i] for i in list(indices_wrong[0])]\n",
    "    zipped_correct = [ zipped[i] for i in list(indices_correct[0])]\n",
    "    \n",
    "    # Wrong Classified\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for i, (img_abs, img_ang, label, pred_label, prob, habit, name) in enumerate(zipped_wrong[:16]):\n",
    "        ax = plt.subplot(8,4, 2*i + 1)\n",
    "        plt.imshow(img_abs, plt.cm.gray)\n",
    "        plt.title(habit +\" - Real:\"+str(int(label))+\" Pred:\"+str(int(pred_label))+\" Prob:\"+str(round(float(prob),2)))\n",
    "        plt.axis(\"off\")\n",
    "        ax = plt.subplot(8,4 , 2*i + 2)\n",
    "        plt.imshow(img_ang, plt.cm.gray)\n",
    "        plt.title(habit +\" - Real:\"+str(int(label))+\" Pred:\"+str(int(pred_label))+\" Prob:\"+str(round(float(prob),2)))\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "    # Right Classified\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for i, (img_abs, img_ang, label, pred_label, prob, habit, name) in enumerate(zipped_correct[:16]):\n",
    "        ax = plt.subplot(8,4, 2*i + 1)\n",
    "        plt.imshow(img_abs, plt.cm.gray)\n",
    "        plt.title(habit +\" - Real:\"+str(int(label))+\" Pred:\"+str(int(pred_label))+\" Prob:\"+str(round(float(prob),2)))\n",
    "        plt.axis(\"off\")\n",
    "        ax = plt.subplot(8,4, 2*i + 2)\n",
    "        plt.imshow(img_ang, plt.cm.gray)\n",
    "        plt.title(habit +\" - Real:\"+str(int(label))+\" Pred:\"+str(int(pred_label))+\" Prob:\"+str(round(float(prob),2)))\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "    \n",
    "    # find out how many particles belong to what class\n",
    "    \n",
    "    # find out size of particles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all parameters for each try in this dict, log later together with results\n",
    "run = {\n",
    "    # general\n",
    "    \"dataset\" : \"uncropped_oldLabelling\",\n",
    "    \"save\": True,\n",
    "    \"verbose\": 1,\n",
    "    \"plot\": plot_all,\n",
    "    \n",
    "    # preprocessing\n",
    "    \"balance_dataset\": \"False\", #\"down_sampling\" or \"up_sampling\" for True\n",
    "                                      # or \"augment\" for upsampling with augmentation\n",
    "    \"label\": \"label_proc_aggregate\",\n",
    "    \"normalize\": normalize1,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"BUFFER_SIZE\": 2048, # try bigger!\n",
    "    \"preprocessing\": preprocess_data,\n",
    "\n",
    "    \n",
    "    # model compile\n",
    "    \"model\": getModelDenseNet121,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"optimizer\": tf.keras.optimizers.Adam,\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \n",
    "    # do the ones below actually work?\n",
    "    \"pretrained\": True, #train_basemodel set to false, except if nohead Model\n",
    "    \"weight\": False,\n",
    "    \"initial_bias\": False,\n",
    "    \"early_stopping\": False,\n",
    "    #\"freeze_basemodel\": True, # not implemented\n",
    "    \n",
    "    # model fit\n",
    "    \"epochs\": 2,\n",
    "    \n",
    "    # run test\n",
    "    \"run_test\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'uncropped_oldLabelling', 'save': True, 'verbose': 1, 'plot': <function plot_all at 0x7f3517402430>, 'balance_dataset': 'False', 'label': 'label_proc_aggregate', 'normalize': <function normalize1 at 0x7f3517668b80>, 'BATCH_SIZE': 32, 'BUFFER_SIZE': 2048, 'preprocessing': <function preprocess_data at 0x7f3517668e50>, 'model': <function getModelDenseNet121 at 0x7f3517661160>, 'learning_rate': 0.0001, 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'loss': 'binary_crossentropy', 'pretrained': True, 'weight': False, 'initial_bias': False, 'early_stopping': False, 'epochs': 2, 'run_test': True}\n",
      "Load data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/train/uncropped_oldLabelling/train_set_128px.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_112623/2106315287.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# batch size = 32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#exp1_fine = run_experiment_finetune(run, exp1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_112623/3515185886.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(run, continue_training)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# start new model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mY_train\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preprocessing\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mval_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preprocessing\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_112623/3461997654.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(run)\u001b[0m\n\u001b[1;32m      6\u001b[0m     }\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# use k fold cross validation later :)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#data = data.loc[data['label_habit'] == \"Column\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \"\"\"\n\u001b[1;32m    195\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/train/uncropped_oldLabelling/train_set_128px.pkl'"
     ]
    }
   ],
   "source": [
    "exp1 = run_experiment(run, False) # batch size = 32\n",
    "#exp1_fine = run_experiment_finetune(run, exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run[\"learning_rate\"] *= 0.1\n",
    "#exp1 = run_experiment(run, exp1)\n",
    "#del exp1\n",
    "#run[\"dataset\"] = \"cropped_oldLabelling\"\n",
    "#exp2 = run_experiment(run, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run[\"label\"] = \"label_proc_aged\"\n",
    "#exp3 = run_experiment(run, False)\n",
    "#del exp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run[\"label\"] = \"label_proc_aggregate\"\n",
    "#exp3 = run_experiment(run, False)\n",
    "#del exp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run[\"learning_rate\"] = 0.005\n",
    "#exp2 = run_experiment(run, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del exp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp5 = run_experiment(run, False) # all, nohead, weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
