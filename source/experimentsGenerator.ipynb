{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#import tensorflow_addons as tfa\n",
    "import keras.backend as K\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngpus = tf.config.list_physical_devices(\\'GPU\\')\\nif gpus:\\n    try:\\n        # Currently, memory growth needs to be the same across GPUs\\n        for gpu in gpus:\\n            tf.config.experimental.set_memory_growth(gpu, True)\\n        logical_gpus = tf.config.list_logical_devices(\\'GPU\\')\\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\\n    except RuntimeError as e:\\n        # Memory growth must be set before GPUs have been initialized\\n        print(e)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed before each experiment -> reproducible results\n",
    "def reset_random():\n",
    "    seed = 7655\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    rng = np.random.default_rng()\n",
    "reset_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(run):\n",
    "    #print(\"Load data (split for experiment)\")\n",
    "    path_dictionary = {\n",
    "        \"cropped_oldLabelling\" : \"../data/train/cropped_oldLabelling/train_set_128px.pkl\",\n",
    "        \"uncropped_oldLabelling\" : \"../data/train/uncropped_oldLabelling/train_set_128px.pkl\"\n",
    "    }\n",
    "    \n",
    "    data = pd.read_pickle(path_dictionary[run[\"dataset\"]])\n",
    "    # use k fold cross validation later :)\n",
    "    #data = data.loc[data['label_habit'] == \"Column\"]\n",
    "    #print(len(data))\n",
    "    #train_data, val_data = train_test_split(data, stratify=data[\"label_proc_rimed\"], test_size=0.25, random_state=7655)\n",
    "    #del data\n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=7655).split(data, data[run[\"label\"]])\n",
    "    for idx in range(run[\"fold\"]+1):\n",
    "        result = next(kf, None)\n",
    "        #print(run[\"fold\"], idx, result)\n",
    "        #print(len(result[0]), len(result[1]))\n",
    "    \n",
    "\n",
    "    train_data = data.iloc[result[0]]\n",
    "    val_data = data.iloc[result[1]]\n",
    "    del data\n",
    "    #return train_data[:500], val_data[:100]\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_test(run):\n",
    "    print(\"Load data test\")\n",
    "    path_dictionary = {\n",
    "        \"cropped_oldLabelling\" : \"../data/test/cropped_oldLabelling/test_set_128px.pkl\",\n",
    "        \"uncropped_oldLabelling\" : \"../data/test/uncropped_oldLabelling/test_set_128px.pkl\"\n",
    "    }    \n",
    "    data = pd.read_pickle(path_dictionary[run[\"dataset\"]])\n",
    "    #return data[:20]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_train(run):\n",
    "    print(\"Load data train\")\n",
    "    path_dictionary = {\n",
    "        \"cropped_oldLabelling\" : \"../data/train/cropped_oldLabelling/train_set_128px.pkl\",\n",
    "        \"uncropped_oldLabelling\" : \"../data/train/uncropped_oldLabelling/train_set_128px.pkl\"\n",
    "    }    \n",
    "    data = pd.read_pickle(path_dictionary[run[\"dataset\"]])\n",
    "    #return data[:100]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test(run):\n",
    "    #print(\"Load train test\")\n",
    "    path_dictionary_test = {\n",
    "        \"cropped_oldLabelling\" : \"../data/test/cropped_oldLabelling/test_set_128px.pkl\",\n",
    "        \"uncropped_oldLabelling\" : \"../data/test/uncropped_oldLabelling/test_set_128px.pkl\"\n",
    "    }  \n",
    "    test_df = pd.read_pickle(path_dictionary_test[run[\"dataset\"]])\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=7655).split(test_df, test_df[run[\"label\"]])\n",
    "    for idx in range(run[\"fold\"]+1):\n",
    "        result = next(kf, None)\n",
    "    \n",
    "\n",
    "    train_data_fromTest = test_df.iloc[result[0]]\n",
    "    test_data = test_df.iloc[result[1]]\n",
    "    \n",
    "    path_dictionary_train = {\n",
    "        \"cropped_oldLabelling\" : \"../data/train/cropped_oldLabelling/train_set_128px.pkl\",\n",
    "        \"uncropped_oldLabelling\" : \"../data/train/uncropped_oldLabelling/train_set_128px.pkl\"\n",
    "    }    \n",
    "    train_data_fromTrain = pd.read_pickle(path_dictionary_train[run[\"dataset\"]])\n",
    "    train_data = pd.concat([train_data_fromTest,train_data_fromTrain],ignore_index=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    #return train_data[:500], test_data[:200]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to mean = 0, std = 1\n",
    "def standardize(images, ds_type):\n",
    "    mean = images.mean(axis=(1,2), keepdims=True)\n",
    "    std = images.std(axis=(1,2), keepdims=True)\n",
    "    images = (images - mean) / std\n",
    "    return images\n",
    "\n",
    "# transform into range [0,1]\n",
    "def scale(images, ds_type):\n",
    "    minimum = np.min(images, axis=(1,2), keepdims=True)\n",
    "    maximum = np.max(images, axis=(1,2), keepdims=True)\n",
    "    return (images - minimum) / (maximum-minimum)\n",
    "\n",
    "# transform into range [-1,1]\n",
    "def normalize2(images, ds_type):\n",
    "    return (2*scale(images))-1\n",
    "\n",
    "# transform like in https://towardsdatascience.com/data-preprocessing-and-network-building-in-cnn-15624ef3a28b -> Normalization\n",
    "def normalize3(images, ds_type):\n",
    "    minimum = np.min(images, axis=(1,2), keepdims=True)\n",
    "    maximum = np.max(images, axis=(1,2), keepdims=True)\n",
    "    return images - (minimum / maximum) - minimum\n",
    "\n",
    "def centering(images, ds_type):\n",
    "    mean = np.mean(images, axis=(1,2), keepdims=True)\n",
    "    return scale(images-mean)\n",
    "\n",
    "def normalize_and_standardize(images, ds_type):\n",
    "    images = normalize2(images)\n",
    "    mean = np.mean(images)\n",
    "    std = np.std(images)\n",
    "    images = (images - mean) / std\n",
    "    return images\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(data, ds_type, run):\n",
    "    #print(\"Preprocess data for 3 channels\", end=\" \")\n",
    "    \n",
    "    X_abs = run[\"normalize\"](np.stack(data[\"img_abs\"]), ds_type)\n",
    "    X_ang = run[\"normalize\"](np.stack(data[\"img_ang\"]), ds_type)\n",
    "    X = np.stack((X_abs, X_abs, X_ang), axis=-1)\n",
    "    Y = data[run[\"label\"]].to_numpy()\n",
    "    del X_abs, X_ang, data\n",
    "    \n",
    "    n_tot = len(Y)\n",
    "    n_pos = np.sum(Y)\n",
    "    n_neg = n_tot - n_pos\n",
    "    #print(n_pos / n_tot, n_neg / n_tot)\n",
    "    \n",
    "    if ds_type == \"train\":\n",
    "        if run[\"balance_dataset\"] == \"down_sampling\":\n",
    "            X, Y = down_sample(X, Y)    \n",
    "        elif run[\"balance_dataset\"] == \"up_sampling\":\n",
    "            X, Y = up_sample(X, Y)\n",
    "        elif run[\"balance_dataset\"] == \"augment\":\n",
    "            X, Y = up_sample_augment(X, Y)\n",
    "    \n",
    "\n",
    "    datagenerator = ImageDataGenerator().flow(X,Y,batch_size=run[\"BATCH_SIZE\"], shuffle=False)\n",
    "    \n",
    "    del X\n",
    "    #print(\"Finish preprocessing\")\n",
    "\n",
    "    return datagenerator, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sample(X_train, Y_train):\n",
    "    n_tot = len(Y_train)\n",
    "    n_pos = np.sum(Y_train)\n",
    "    n_neg = n_tot - n_pos\n",
    "    print(n_tot, n_pos, n_neg)\n",
    "    print(n_neg/(n_neg-n_pos))\n",
    "    \n",
    "    # Undersample Data -> Balance it\n",
    "    neg_idx = np.where(Y_train==0)[0]\n",
    "    #print(neg_idx.shape)\n",
    "    print(neg_idx)\n",
    "    print(type(neg_idx))\n",
    "    idx_del = np.random.choice(n_neg, size=n_neg-n_pos, replace=False)\n",
    "    Y_train = np.delete(Y_train, neg_idx[idx_del])\n",
    "    X_train = np.delete(X_train, neg_idx[idx_del], axis=0)\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_sample(X_train, Y_train):\n",
    "    n_tot = len(Y_train)\n",
    "    n_pos = np.sum(Y_train)\n",
    "    n_neg = n_tot - n_pos\n",
    "    print(n_tot, n_pos, n_neg)\n",
    "    #print(n_neg/(n_neg-n_pos))\n",
    "    \n",
    "    if n_pos == 0:\n",
    "        print(\"Warning!!!!, upsampling with no positive samples! (func: up_sample)\")\n",
    "        return X_train, Y_train\n",
    "    # Oversample Data -> Balance it\n",
    "    times_whole_positive = n_neg // n_pos\n",
    "    extra_positive = n_neg % n_pos\n",
    "    print(times_whole_positive, extra_positive)\n",
    "        \n",
    "    pos_idx = np.where(Y_train==1)[0]\n",
    "    Y_train_pos = Y_train[pos_idx]\n",
    "    X_train_pos = X_train[pos_idx]\n",
    "    \n",
    "    for i in range(times_whole_positive-1):\n",
    "        Y_train = np.concatenate((Y_train, Y_train_pos), axis=0)\n",
    "        X_train = np.concatenate((X_train, X_train_pos), axis=0)\n",
    "        \n",
    "    Y_train = np.concatenate((Y_train, Y_train_pos[:extra_positive]), axis=0)\n",
    "    X_train = np.concatenate((X_train, X_train_pos[:extra_positive]), axis=0)\n",
    "    \n",
    "    del Y_train_pos, X_train_pos\n",
    "    \n",
    "    assert len(Y_train) == len(X_train)\n",
    "    p = np.random.permutation(len(Y_train))\n",
    "    Y_train = Y_train[p]\n",
    "    X_train = X_train[p]\n",
    "    \n",
    "    n_tot = len(Y_train)\n",
    "    n_pos = np.sum(Y_train)\n",
    "    n_neg = n_tot - n_pos\n",
    "    print(n_tot, n_pos, n_neg)\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_sample_augment(X_train, Y_train):\n",
    "    # total length is going to be 8*len(smaller class) (8=4*rotation,flip, 4*rotation again)\n",
    "    n_tot = len(Y_train)\n",
    "    n_pos = np.sum(Y_train)\n",
    "    n_neg = n_tot - n_pos\n",
    "    print(n_tot, n_pos, n_neg)\n",
    "    # 1356 117 1239\n",
    "    #print(n_neg/(n_neg-n_pos))\n",
    "        \n",
    "    pos_idx = np.where(Y_train==1)[0]\n",
    "    Y_train_pos = Y_train[pos_idx]\n",
    "    X_train_pos = X_train[pos_idx]\n",
    "    \n",
    "    neg_idx = np.where(Y_train==0)[0]\n",
    "    Y_train_neg = Y_train[neg_idx]\n",
    "    X_train_neg = X_train[neg_idx]\n",
    "\n",
    "    del X_train, Y_train\n",
    "    \n",
    "    X_train_pos, Y_train_pos = rotate_and_flip(X_train_pos, Y_train_pos)\n",
    "    X_train_neg, Y_train_neg = rotate_and_flip(X_train_neg, Y_train_neg, n=n_pos*8)\n",
    "    \n",
    "    Y_train = np.concatenate((Y_train_neg, Y_train_pos), axis=0)\n",
    "    X_train = np.concatenate((X_train_neg, X_train_pos), axis=0)\n",
    "    del Y_train_neg, Y_train_pos, X_train_neg, X_train_pos\n",
    "\n",
    "    assert len(Y_train) == len(X_train)\n",
    "    p = np.random.permutation(len(Y_train))\n",
    "    Y_train = Y_train[p]\n",
    "    X_train = X_train[p]\n",
    "    \n",
    "    n_tot = len(Y_train)\n",
    "    n_pos = np.sum(Y_train)\n",
    "    n_neg = n_tot - n_pos\n",
    "    print(n_tot, n_pos, n_neg)\n",
    "    return X_train, Y_train\n",
    "# 1872 27862362.0 -27860490.0\n",
    "\n",
    "def rotate_and_flip(X_train, Y_train, n=0):\n",
    "    print(\"rotate and flip\")\n",
    "    X_train = np.concatenate((X_train, np.rot90(X_train, axes=(1, 2))))\n",
    "    X_train = np.concatenate((X_train, np.rot90(X_train, axes=(1, 2), k=2)))\n",
    "    #X_train = np.concatenate((X_train, np.flip(X_train)))\n",
    "    X_train = np.concatenate((X_train, np.flipud(X_train)))\n",
    "    \n",
    "    Y_train = np.tile(Y_train,8)\n",
    "    \n",
    "    if n > 0:\n",
    "        # random subsample to n\n",
    "        n_tot = len(Y_train)\n",
    "        idx_del = np.random.choice(n_tot, size=n_tot-n, replace=False)\n",
    "        Y_train = np.delete(Y_train, idx_del)\n",
    "        X_train = np.delete(X_train, idx_del, axis=0)\n",
    "        \n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelDenseNet121(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "        \n",
    "    base_model = tf.keras.applications.densenet.DenseNet121(\n",
    "        include_top=False, weights=pretrained_weights,\n",
    "        input_shape=IMG_SHAPE, pooling='max')\n",
    "    \n",
    "    base_model.trainable = run[\"base_model_trainable\"]    \n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    x = base_model(inputs, training=True)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model, base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelDenseNet121_noHead(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "        \n",
    "    base_model = tf.keras.applications.densenet.DenseNet121(\n",
    "        include_top=False, weights=pretrained_weights,\n",
    "        input_shape=IMG_SHAPE, pooling='max')\n",
    "    \n",
    "    base_model.trainable = run[\"base_model_trainable\"]\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    x = base_model(inputs, training=True)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model, base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelDenseNet201(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "    base_model = tf.keras.applications.densenet.DenseNet201(\n",
    "        include_top=False, weights=pretrained_weights,\n",
    "        input_shape=IMG_SHAPE, pooling='max')\n",
    "    \n",
    "    base_model.trainable = run[\"base_model_trainable\"]\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    x = base_model(inputs, training=True)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelDenseNet201_noHead(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "    base_model = tf.keras.applications.densenet.DenseNet201(\n",
    "        include_top=False, weights=pretrained_weights,\n",
    "        input_shape=IMG_SHAPE, pooling='max')\n",
    "    \n",
    "    base_model.trainable = run[\"base_model_trainable\"]\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    x = base_model(inputs, training=True)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-center each color channel with respect to the ImageNet dataset, without scaling.\n",
    "def getModelResNet50(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        include_top=False, weights=pretrained_weights,\n",
    "        input_shape=IMG_SHAPE, pooling='max')\n",
    "    \n",
    "    base_model.trainable = run[\"base_model_trainable\"]\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    x = base_model(inputs, training=True)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-center each color channel with respect to the ImageNet dataset, without scaling.\n",
    "def getModelResNet50_noHead(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        include_top=False, weights=pretrained_weights,\n",
    "        input_shape=IMG_SHAPE, pooling='max')\n",
    "    \n",
    "    base_model.trainable = run[\"base_model_trainable\"]\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    x = base_model(inputs, training=True)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-center each color channel with respect to the ImageNet dataset, without scaling.\n",
    "def getModelResNet152(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "    base_model = tf.keras.applications.ResNet152(\n",
    "        include_top=False, weights=pretrained_weights,\n",
    "        input_shape=IMG_SHAPE, pooling='max')\n",
    "    \n",
    "    base_model.trainable = run[\"base_model_trainable\"]\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    x = base_model(inputs, training=True)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-center each color channel with respect to the ImageNet dataset, without scaling.\n",
    "def getModelResNet152_noHead(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "    base_model = tf.keras.applications.ResNet152(\n",
    "        include_top=False, weights=pretrained_weights,\n",
    "        input_shape=IMG_SHAPE, pooling='max')\n",
    "    \n",
    "    base_model.trainable = run[\"base_model_trainable\"]\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    x = base_model(inputs, training=True)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelMobileNetV2(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights=pretrained_weights, pooling='max')\n",
    "    \n",
    "    base_model.trainable = run[\"base_model_trainable\"]\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    x = base_model(inputs, training=True)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelMobileNetV2_nohead(run):\n",
    "    IMG_SHAPE = (128, 128, 3)\n",
    "    if run[\"pretrained\"]:\n",
    "        pretrained_weights = 'imagenet'\n",
    "    else:\n",
    "        pretrained_weights = None\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights=pretrained_weights, pooling='max')\n",
    "    \n",
    "    base_model.trainable = run[\"base_model_trainable\"]\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    x = base_model(inputs, training=True)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(run):\n",
    "    initial_bias = run[\"initial_bias\"]\n",
    "    if initial_bias:\n",
    "        initial_bias = np.log([642/3551])\n",
    "    #print(\"Make model\",initial_bias)\n",
    "\n",
    "    model, base_model = run[\"model\"](run)\n",
    "    #model.summary()\n",
    "    \n",
    "    model.compile(optimizer = run[\"optimizer\"](learning_rate=run[\"learning_rate\"]),\n",
    "                  loss = run[\"loss\"],\n",
    "                  metrics = ['binary_accuracy',\n",
    "                             'hinge',\n",
    "                             tf.keras.metrics.AUC(name='auc'),\n",
    "                             tf.keras.metrics.Recall(name='recall'),\n",
    "                             tf.keras.metrics.Precision(name='precision'),\n",
    "                            F1Score(threshold=0.5, num_classes=1, name='f1_score')]\n",
    "             )\n",
    "    return model, base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(image, label):\n",
    "    #true_frac = 0.14963822851236383\n",
    "    #weights = tf.constant([true_frac, 1.0 - true_frac])\n",
    "    #weights = tf.constant([5.68 / 6.68, 1.0 / 6.68]) \n",
    "    weights = tf.constant([0.16299749633082966, 0.83700250366917])\n",
    "    sample_weights = tf.gather(weights, indices=tf.cast(label, tf.int64))\n",
    "    return image, label, sample_weights\n",
    "\n",
    "def train_model(model, train_generator, val_generator, run):\n",
    "    #print(\"Train\")\n",
    "    #if run[\"weight\"]:\n",
    "    #    history = model.fit(train_batches.map(weights), epochs=run[\"epochs\"], \n",
    "    #                validation_data=val_batches, verbose=run[\"verbose\"])\n",
    "    #else:\n",
    "    \n",
    "    monitor = 'val_f1_score'\n",
    "               \n",
    "    #TODO: change patience to 5!!!!\n",
    "    if run[\"early_stopping\"]:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=5, verbose=1,mode='max')\n",
    "        history = model.fit(train_generator, epochs=run[\"epochs\"], \n",
    "                    validation_data=val_generator, verbose=run[\"verbose\"], callbacks=[callback])\n",
    "        \n",
    "        loss_hist = history.history[monitor]\n",
    "        run[\"determined_epochs\"] = np.argmax(loss_hist) + 1\n",
    "        if run[\"determined_epochs\"] == run[\"epochs\"]:\n",
    "            print(\"\\n ########## Warning, not early stopped ##########\")\n",
    "        print(\"determined epoch: \"+str(run[\"determined_epochs\"]))\n",
    "        \n",
    "    else:\n",
    "        history = model.fit(train_generator, epochs=run[\"epochs\"], \n",
    "                        validation_data=val_generator, verbose=run[\"verbose\"])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history,Y_pred_prob, Y_val, number=None):\n",
    "    plt.plot(history.history['binary_accuracy'])\n",
    "    plt.plot(history.history['val_binary_accuracy'])\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.plot(history.history['hinge'])\n",
    "    plt.plot(history.history['val_hinge'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['accuracy', 'val_accuracy', 'loss', 'val_loss', 'hinge', 'val_hinge'], loc='upper left')\n",
    "    \n",
    "    if number:\n",
    "        plt.savefig(\"../logs/plot/model\"+str(run[\"number\"])+\"_\"+str(run[\"fold\"])+\"metrics.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raw_metrics(history,Y_pred_prob, Y_val, number=None):\n",
    "    prec = np.array(history.history['precision'])\n",
    "    rec = np.array(history.history['recall'])\n",
    "    val_prec = np.array(history.history['val_precision'])\n",
    "    val_rec = np.array(history.history['val_recall'])\n",
    "\n",
    "    # avoid division by zero\n",
    "    tol = 10e-7   \n",
    "    summe = prec + rec\n",
    "    val_summe = val_prec + val_rec\n",
    "    \n",
    "    summe = np.where(summe == 0.0, tol, summe)\n",
    "    val_summe = np.where(val_summe == 0.0, tol, val_summe)\n",
    "    \n",
    "    f1 = 2 * (prec * rec) / summe\n",
    "    val_f1 = 2 * (val_prec * val_rec) / val_summe\n",
    "    \n",
    "    plt.plot(history.history['recall'])\n",
    "    plt.plot(history.history['val_recall'])\n",
    "    plt.plot(history.history['precision'])\n",
    "    plt.plot(history.history['val_precision'])\n",
    "    plt.plot(f1)\n",
    "    plt.plot(val_f1)\n",
    " \n",
    "    plt.title('Recall & Precision')\n",
    "    plt.ylabel('rate')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['recall', 'val_recall', 'precision', 'val_precision', 'f1', 'val_f1'], loc='upper left')\n",
    "    if number:\n",
    "        plt.savefig(\"../logs/plot/model\"+str(run[\"number\"])+\"_\"+str(run[\"fold\"])+\"raw_metrics.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC(history,Y_pred_prob, Y_val, number=None):\n",
    "    fpr, tpr, threshold = metrics.roc_curve(Y_val, Y_pred_prob)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    if number:\n",
    "        plt.savefig(\"../logs/plot/model\"+str(run[\"number\"])+\"_\"+str(run[\"fold\"])+\"roc.png\")\n",
    "    plt.show()\n",
    "    print(\"AUC = \"+ str(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(history,Y_pred_prob, Y_val, number=None):\n",
    "    plot_metrics(history,Y_pred_prob, Y_val, number)\n",
    "    plot_raw_metrics(history,Y_pred_prob, Y_val, number)\n",
    "    plot_ROC(history,Y_pred_prob, Y_val, number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, val_generator, run):\n",
    "    print(\"Make predictions\")\n",
    "    Y_pred_prob = model.predict(val_generator)\n",
    "    print(Y_pred_prob.shape)\n",
    "    print(Y_pred_prob)\n",
    "    print(np.sum(Y_pred_prob))\n",
    "    # TODO: change np where !\n",
    "    #Y_pred = np.argmax(Y_pred_prob, axis=1)\n",
    "    print(\"YPredProb \"+str(Y_pred_prob.shape))\n",
    "    print(Y_pred_prob)\n",
    "    Y_pred = np.where(Y_pred_prob < 0.5, 0, 1)[:,0]\n",
    "    print(\"###############\", Y_pred.shape)\n",
    "    print(Y_pred)\n",
    "    print(np.sum(Y_pred))\n",
    "    return Y_pred, Y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluate and save evaluation and run in a logfile\n",
    "def evaluate_model(y_true, y_pred, y_pred_prob, history, val_generator, run):\n",
    "    #model_summary = history.model.summary()\n",
    "    params = history.params\n",
    "    datestr = time.strftime(\"%y:%m:%d\")\n",
    "    timestr = time.strftime(\"%H:%M:%S\")\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    conf_mat_percent = 100 * conf_mat / len(y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bce = log_loss(y_true, y_pred)\n",
    "\n",
    "    prediction_prob = y_pred_prob\n",
    "    prediction_bool = y_pred\n",
    "    Y_test = y_true\n",
    "    \n",
    "    conf_mat = confusion_matrix(Y_test, prediction_bool)\n",
    "    conf_mat_percent = 100 * conf_mat / len(prediction_bool)\n",
    "    print(conf_mat)\n",
    "    print(conf_mat_percent)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Acc: \"+str(accuracy_score(Y_test, prediction_bool)))\n",
    "    print(\"Bal.Acc: \"+str(balanced_accuracy_score(Y_test, prediction_bool)))\n",
    "\n",
    "    print(\"TN: \"+str(conf_mat[0,0]))\n",
    "    print(\"FP: \"+str(conf_mat[0,1]))\n",
    "    print(\"FN: \"+str(conf_mat[1,0]))\n",
    "    print(\"TP: \"+str(conf_mat[1,1]))\n",
    "    \n",
    "    TN = conf_mat[0,0]\n",
    "    FP = conf_mat[0,1]\n",
    "    \n",
    "    print(\"Recall: \"+str(recall_score(Y_test, prediction_bool)))\n",
    "    print(\"Precision: \"+str(precision_score(Y_test, prediction_bool)))   \n",
    "    print(\"F1: \"+str(f1_score(Y_test, prediction_bool)))\n",
    "    print(\"FPR: \"+str(FP / (FP+TN)))\n",
    "    print(\"ROC_AUC: \"+str(roc_auc_score(Y_test, prediction_prob)))\n",
    "    \n",
    "    if run[\"save\"]:       \n",
    "        filename = \"../logs/\"+datestr+\"/\"+timestr+\"_\"+run[\"label\"]+\"_\"+str(run[\"epochs\"])+\".txt\"\n",
    "        # make folder if it doesn't exist already\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        \n",
    "        f = open(filename, \"x\")\n",
    "        # write run specification\n",
    "        f.write(\"Label: \"+run[\"label\"]+\"\\n\")\n",
    "        f.write(\"Normalization: \"+run[\"normalize\"].__name__+\"\\n\")\n",
    "        f.write(\"Batch Size: \"+str(run[\"BATCH_SIZE\"])+\"\\n\")\n",
    "        f.write(\"Buffer Size: \"+str(run[\"BUFFER_SIZE\"])+\"\\n\")\n",
    "        f.write(\"Model Name: \"+run[\"model\"].__name__+\"\\n\")\n",
    "        f.write(\"Optimizer: \"+str(run[\"optimizer\"])+\"\\n\")\n",
    "        f.write(\"Loss: \"+str(run[\"loss\"])+\"\\n\")\n",
    "        f.write(\"Epochs: \"+str(run[\"epochs\"])+\"\\n\")\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        # write evaluation\n",
    "        f.write(\"F1: \"+str(f1)+\"\\n\")\n",
    "        f.write(\"Accuracy: \"+str(acc)+\"\\n\")\n",
    "        f.write(\"Logloss: \"+str(bce)+\"\\n\\n\")\n",
    "        \n",
    "        f.write(str(conf_mat)+\"\\n\\n\")\n",
    "        f.write(str(conf_mat_percent)+\"\\n\\n\")\n",
    "        \n",
    "        # write model summary\n",
    "        f.write(str(history.params) + \"\\n\\n\")\n",
    "        #f.write(str(model_summary))\n",
    "        history.model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "        f.close()\n",
    "        \n",
    "        # Get the dictionary containing each metric and the loss for each epoch\n",
    "        history_filename = \"../logs/history/\"+datestr+\"/\"+timestr+\"_\"+run[\"label\"]+\"_\"+str(run[\"epochs\"])+\".txt\"\n",
    "        os.makedirs(os.path.dirname(history_filename), exist_ok=True)\n",
    "        history_dict = history.history\n",
    "        json.dump(history_dict, open(history_filename, 'w'))\n",
    "    \n",
    "    #find_misclassified(y_true, y_pred, y_pred_prob, val_generator, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_bias(y):\n",
    "    pos = np.sum(y)\n",
    "    neg = np.sum(1-y)\n",
    "    initial_bias = np.log([pos/neg])\n",
    "    return initial_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(run):\n",
    "    #print(run)\n",
    "    reset_random()\n",
    "    \n",
    "    train_data, test_data = load_train_test(run)\n",
    "    \n",
    "    train_generator, _ = preprocess_data(train_data, \"train\", run)\n",
    "    del train_data\n",
    "    \n",
    "    test_generator, Y_test = preprocess_data(test_data, \"test\", run)\n",
    "    del test_data   \n",
    "    \n",
    "    model, base_model = make_model(run)\n",
    "\n",
    "    history = model.fit(train_generator, epochs=run[\"epochs\"], validation_data=test_generator, verbose=run[\"verbose\"])    \n",
    "    del train_generator\n",
    "    \n",
    "    # ---------- done training, start testing ---------- #\n",
    "    \n",
    "    \n",
    "\n",
    "    prediction_prob = model.predict(test_generator)\n",
    "    prediction_bool = np.where(prediction_prob < 0.5, 0, 1)[:,0]\n",
    "    \n",
    "    f = open(\"../logs/testlog/model\"+str(run[\"number\"])+\"_\"+str(run[\"fold\"])+\".txt\", \"w\")\n",
    "    \n",
    "    conf_mat = confusion_matrix(Y_test, prediction_bool)\n",
    "    conf_mat_percent = 100 * conf_mat / len(prediction_bool)\n",
    "    f.write(str(conf_mat))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(conf_mat_percent))\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"Acc: \"+str(accuracy_score(Y_test, prediction_bool)))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Bal.Acc: \"+str(balanced_accuracy_score(Y_test, prediction_bool)))\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"TN: \"+str(conf_mat[0,0]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"FP: \"+str(conf_mat[0,1]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"FN: \"+str(conf_mat[1,0]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"TP: \"+str(conf_mat[1,1]))\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    TN = conf_mat[0,0]\n",
    "    FP = conf_mat[0,1]\n",
    "    \n",
    "    f.write(\"Recall: \"+str(recall_score(Y_test, prediction_bool)))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Precision: \"+str(precision_score(Y_test, prediction_bool)))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"F1: \"+str(f1_score(Y_test, prediction_bool)))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"FPR: \"+str(FP / (FP+TN)))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"ROC_AUC: \"+str(roc_auc_score(Y_test, prediction_prob)))\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    \n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Dataset: \"+str(run[\"dataset\"]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Balance: \"+str(run[\"balance_dataset\"]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Label: \"+run[\"label\"])\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Model Name: \"+run[\"model\"].__name__+\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Lr: \"+str(run[\"learning_rate\"]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Optimizer: \"+str(run[\"optimizer\"]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Epochs: \"+str(run[\"epochs\"]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    for x, y in run.items():\n",
    "        f.write(str(x)+\" : \"+str(y))\n",
    "    f.write(\"\\n\")\n",
    "    #f.write(model.summary()) \n",
    "    history.model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "    f.close()\n",
    "    \n",
    "    f = open(\"../logs/testlog/model\"+str(run[\"number\"])+\"_\"+str(run[\"fold\"])+\".txt\", \"r\")\n",
    "    print(f.read())\n",
    "    f.close()\n",
    "    \n",
    "    plot_all(history, prediction_prob, Y_test, run[\"number\"])\n",
    "    \n",
    "    find_misclassified(Y_test, prediction_bool, prediction_prob, test_generator, run, True)\n",
    "    \n",
    "    \n",
    "    if run[\"save_model\"]:\n",
    "        model.save(\"../logs/models/model\"+str(run[\"number\"])+\"_\"+str(run[\"fold\"])+\".h5\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(run):\n",
    "    reset_random()\n",
    "    #if run[\"verbose\"]:\n",
    "        #print(run)\n",
    "        \n",
    "    train_data, val_data = load_data(run)    \n",
    "    train_generator,  Y_train  = preprocess_data(train_data, \"train\", run)\n",
    "    val_generator, Y_val = preprocess_data(val_data, \"val\", run)\n",
    "    del train_data, val_data\n",
    "    \n",
    "    if run[\"initial_bias\"]:\n",
    "        run[\"initial_bias\"] = initial_bias(Y_train)\n",
    "    del Y_train\n",
    "    \n",
    "    model, base_model = make_model(run)    \n",
    "    history = train_model(model, train_generator, val_generator, run)\n",
    "    #Y_pred, Y_pred_prob = predict(model, val_generator, run)\n",
    "    \n",
    "    #run[\"plot\"](history,Y_pred_prob, Y_val)\n",
    "\n",
    "    #evaluate_model(Y_val, Y_pred, Y_pred_prob, history, val_generator, run)\n",
    "    \n",
    "    return train_generator, val_generator, Y_val, model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_finetune(run, continue_training=False):\n",
    "    reset_random()\n",
    "    #if run[\"verbose\"]:\n",
    "        #print(run)\n",
    "        \n",
    "    # start new model\n",
    "    if not continue_training:\n",
    "        train_data, val_data = load_data(run)    \n",
    "        train_batches,  Y_train  = preprocess_data(train_data, \"train\", run)\n",
    "        val_batches, Y_val = preprocess_data(val_data, \"val\", run)\n",
    "        del train_data, val_data\n",
    "        if run[\"initial_bias\"]:\n",
    "            run[\"initial_bias\"] = initial_bias(Y_train)\n",
    "        del Y_train\n",
    "        model, base_model = make_model(run)\n",
    "        epochs_already_done = 0\n",
    "    \n",
    "    #continue training    \n",
    "    else:\n",
    "        run[\"initial_bias\"] = False\n",
    "        train_batches, val_batches, Y_val, model, old_history = continue_training\n",
    "        epochs_already_done = old_history.params[\"epochs\"]\n",
    "        \n",
    "    history = train_model(model, train_batches, val_batches, run)    \n",
    "    Y_pred, Y_pred_prob = predict(model, val_batches, run)\n",
    "    run[\"plot\"](history,Y_pred_prob, Y_val)\n",
    "    \n",
    "    run[\"epochs\"] += epochs_already_done\n",
    "    evaluate_model(Y_val, Y_pred, Y_pred_prob, history, val_batches, run)\n",
    "    \n",
    "    base_model.trainable = True\n",
    "    # Let's take a look to see how many layers are in the base model\n",
    "    print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_at = 100\n",
    "\n",
    "    # Freeze all the layers before the `fine_tune_at` layer\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model.compile(optimizer = run[\"optimizer\"](learning_rate=run[\"learning_rate\"]/10),\n",
    "                  loss = run[\"loss\"],\n",
    "                  metrics = ['binary_accuracy',\n",
    "                             'hinge',\n",
    "                             tf.keras.metrics.AUC(name='auc'),\n",
    "                             tf.keras.metrics.Recall(name='recall'),\n",
    "                             tf.keras.metrics.Precision(name='precision')]\n",
    "             )\n",
    "    model.summary()\n",
    "    \n",
    "    fine_tune_epochs = 50\n",
    "    total_epochs = fine_tune_epochs + run[\"epochs\"]\n",
    "    \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=20, verbose=1,restore_best_weights=False)\n",
    "    history_fine = model.fit(train_batches, epochs=total_epochs, \n",
    "                    validation_data=val_batches, verbose=run[\"verbose\"], callbacks=[callback],\n",
    "                            initial_epoch=history.epoch[-1])\n",
    "    \n",
    "    Y_pred, Y_pred_prob = predict(model, val_batches, run)\n",
    "    run[\"plot\"](history,Y_pred_prob, Y_val)\n",
    "    \n",
    "    run[\"epochs\"] += epochs_already_done\n",
    "    evaluate_model(Y_val, Y_pred, Y_pred_prob, history, run)\n",
    "    \n",
    "    return train_batches, val_batches, Y_val, model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_misclassified(y_true, y_pred, y_pred_prob, val_generator, run, save=None):\n",
    "    wrong = np.square(y_true-y_pred) # 0 if true, 1 if false\n",
    "    indices_wrong = np.where(wrong == 1)\n",
    "    indices_correct = np.where(wrong == 0)\n",
    "    \n",
    "    if save:\n",
    "        #val_data = load_data_test(run)\n",
    "        _, val_data = load_train_test(run)\n",
    "    else:\n",
    "        _, val_data = load_data(run)\n",
    "\n",
    "    \n",
    "    basic_habit_dict_all = { \n",
    "        \"Column\": 0,\n",
    "        \"Plate\": 0,\n",
    "        \"Droplet\": 0,\n",
    "        \"Lollipop\": 0,\n",
    "        \"Irregular\": 0,\n",
    "        \"Small\": 0,\n",
    "        \"Plate Column\": 0\n",
    "    }    \n",
    "        \n",
    "    for basic_habit in val_data[\"label_habit\"]:\n",
    "        basic_habit_dict_all[basic_habit] += 1\n",
    "    \n",
    "    array_all = np.array(list(basic_habit_dict_all.values()),dtype=np.float32)\n",
    "        \n",
    "    wrong_data = val_data.iloc[indices_wrong]    \n",
    "    basic_habit_dict_wrong = { \n",
    "        \"Column\": 0,\n",
    "        \"Plate\": 0,\n",
    "        \"Droplet\": 0,\n",
    "        \"Lollipop\": 0,\n",
    "        \"Irregular\": 0,\n",
    "        \"Small\": 0,\n",
    "        \"Plate Column\": 0\n",
    "    }\n",
    "    \n",
    "    for basic_habit in wrong_data[\"label_habit\"]:\n",
    "        basic_habit_dict_wrong[basic_habit] += 1\n",
    "        \n",
    "        \n",
    "    basic_habit_dict_percent = {}\n",
    "    for key in basic_habit_dict_all.keys():\n",
    "        if basic_habit_dict_all[key] != 0:\n",
    "            basic_habit_dict_percent[key] = round(100 * float(basic_habit_dict_wrong[key]) / float(basic_habit_dict_all[key]),2)\n",
    "        else:\n",
    "            basic_habit_dict_percent[key] = 0.0\n",
    "        \n",
    "    array_wrong = np.array(list(basic_habit_dict_wrong.values()),dtype=np.float32)\n",
    "   \n",
    "    correct_data = val_data.iloc[indices_correct] \n",
    "    \n",
    "    print(\"All: \",  basic_habit_dict_all)\n",
    "    print(\"Wrong: \",  basic_habit_dict_wrong)\n",
    "    # avoid division by zero!\n",
    "    print(\"Wrong in %: \", basic_habit_dict_percent)\n",
    "    \n",
    "    if save:\n",
    "        f = open(\"../logs/testlog/model\"+str(run[\"number\"])+\"_\"+str(run[\"fold\"])+\".txt\", \"a\") \n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"All:\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(json.dumps(basic_habit_dict_all))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Wrong:\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(json.dumps(basic_habit_dict_wrong))\n",
    "        f.write(\"\\n\")\n",
    "        f.close()\n",
    "    \n",
    "\n",
    "    #zipped = list(zip(val_batches_images, val_batches_label, y_pred, y_pred_prob, val_data[\"label_habit\"]))\n",
    "    zipped = list(zip(val_data[\"img_abs\"], val_data[\"img_ang\"], val_data[run[\"label\"]], y_pred, y_pred_prob, val_data[\"label_habit\"], val_data.index.values))\n",
    "    #zipped = list(zip(val_data[\"img_abs\"], val_data[\"img_ang\"], val_batches_label, y_pred, y_pred_prob, val_data[\"label_habit\"], val_data.index.values))\n",
    "    zipped_wrong = [ zipped[i] for i in list(indices_wrong[0])]\n",
    "    zipped_correct = [ zipped[i] for i in list(indices_correct[0])]\n",
    "\n",
    "    # Wrong Classified\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for i, (img_abs, img_ang, label, pred_label, prob, habit, name) in enumerate(zipped_wrong[:16]):\n",
    "        ax = plt.subplot(8,4, 2*i + 1)\n",
    "        plt.imshow(img_abs, plt.cm.gray)\n",
    "        plt.title(habit +\" - Real:\"+str(int(label))+\" Pred:\"+str(int(pred_label))+\" Prob:\"+str(round(float(prob),2)))\n",
    "        plt.axis(\"off\")\n",
    "        ax = plt.subplot(8,4 , 2*i + 2)\n",
    "        plt.imshow(img_ang, plt.cm.gray)\n",
    "        plt.title(habit +\" - Real:\"+str(int(label))+\" Pred:\"+str(int(pred_label))+\" Prob:\"+str(round(float(prob),2)))\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "    # Right Classified\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for i, (img_abs, img_ang, label, pred_label, prob, habit, name) in enumerate(zipped_correct[:16]):\n",
    "        ax = plt.subplot(8,4, 2*i + 1)\n",
    "        plt.imshow(img_abs, plt.cm.gray)\n",
    "        plt.title(habit +\" - Real:\"+str(int(label))+\" Pred:\"+str(int(pred_label))+\" Prob:\"+str(round(float(prob),2)))\n",
    "        plt.axis(\"off\")\n",
    "        ax = plt.subplot(8,4, 2*i + 2)\n",
    "        plt.imshow(img_ang, plt.cm.gray)\n",
    "        plt.title(habit +\" - Real:\"+str(int(label))+\" Pred:\"+str(int(pred_label))+\" Prob:\"+str(round(float(prob),2)))\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "    \n",
    "    # find out how many particles belong to what class\n",
    "    \n",
    "    # find out size of particles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all parameters for each try in this dict, log later together with results\n",
    "run = {\n",
    "    # general\n",
    "    \"dataset\" : \"cropped_oldLabelling\",\n",
    "    \"save\": False,\n",
    "    \"verbose\": 1,\n",
    "    \"plot\": plot_all,\n",
    "    \n",
    "    # preprocessing\n",
    "    \"balance_dataset\": False, #\"down_sampling\" or \"up_sampling\" for True\n",
    "                                      # or \"augment\" for upsampling with augmentation\n",
    "    \"label\": \"label_proc_rimed\",\n",
    "    \"normalize\": scale,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"BUFFER_SIZE\": 2048, #2048 try bigger!\n",
    "    \n",
    "    \"fold\": 0,\n",
    "\n",
    "    \n",
    "    # model compile\n",
    "    \"model\": getModelDenseNet201,\n",
    "    \"base_model_trainable\" : True,\n",
    "    \n",
    "    \n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"optimizer\": tf.keras.optimizers.Adam,\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \n",
    "    \"early_stopping\": True,\n",
    "    # do the ones below actually work?\n",
    "    \"pretrained\": True, #train_basemodel set to false, except if nohead Model\n",
    "    \"weight\": False,\n",
    "    \"initial_bias\": False,\n",
    "\n",
    "    #\"freeze_basemodel\": True, # not implemented\n",
    "    \"determined_epochs\": 0,\n",
    "    \n",
    "    # model fit\n",
    "    \"epochs\": 50,\n",
    "    \n",
    "    # save model\n",
    "    \"save_model\": True,\n",
    "    \"number\": 1000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(run, model, number=0):\n",
    "    \n",
    "    run[\"epochs\"] = 50\n",
    "    # default\n",
    "    if number == 0:\n",
    "        run[\"number\"] += 1\n",
    "    else:\n",
    "        run[\"number\"] = number\n",
    "    run[\"model\"] = model\n",
    "    \n",
    "    \n",
    "    determined_epochs = []\n",
    "    for i in range(3):\n",
    "        run[\"fold\"] = i\n",
    "        tf.keras.backend.clear_session()\n",
    "        exp = run_experiment(run)\n",
    "        del exp\n",
    "        determined_epochs.append(run[\"determined_epochs\"])\n",
    "    # take the average and round up!\n",
    "    epoch = math.ceil(np.mean(np.array(determined_epochs)))\n",
    "    run[\"epochs\"] = epoch\n",
    "    print(\"Epochs determined: \" +str(determined_epochs))\n",
    "    print(\"Epoch used for test: \"+str(run[\"epochs\"]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(3):\n",
    "        tf.keras.backend.clear_session()\n",
    "        run[\"fold\"] = i\n",
    "        test = run_test(run)\n",
    "        del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-08 09:16:43.477360: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-08 09:16:44.078255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6553 MB memory:  -> device: 0, name: Quadro P4000, pci bus id: 0000:84:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-08 09:16:56.890791: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 18s 381ms/step - loss: 0.7750 - binary_accuracy: 0.8020 - hinge: 1.1104 - auc: 0.5331 - recall: 0.1020 - precision: 0.0833 - f1_score: 0.0917 - val_loss: 0.1423 - val_binary_accuracy: 0.9600 - val_hinge: 1.0093 - val_auc: 0.8815 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.4172 - binary_accuracy: 0.8940 - hinge: 1.0267 - auc: 0.7187 - recall: 0.1429 - precision: 0.3889 - f1_score: 0.2090 - val_loss: 0.1259 - val_binary_accuracy: 0.9600 - val_hinge: 1.0587 - val_auc: 0.9544 - val_recall: 0.2500 - val_precision: 0.5000 - val_f1_score: 0.3333\n",
      "Epoch 3/50\n",
      "10/16 [=================>............] - ETA: 0s - loss: 0.4851 - binary_accuracy: 0.8409 - hinge: 1.0451 - auc: 0.7675 - recall: 0.2162 - precision: 0.2857 - f1_score: 0.2462"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetModelDenseNet121\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m110\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(run, model, number)\u001b[0m\n\u001b[1;32m     14\u001b[0m run[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m     15\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[0;32m---> 16\u001b[0m exp \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m exp\n\u001b[1;32m     18\u001b[0m determined_epochs\u001b[38;5;241m.\u001b[39mappend(run[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetermined_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(run)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m Y_train\n\u001b[1;32m     15\u001b[0m model, base_model \u001b[38;5;241m=\u001b[39m make_model(run)    \n\u001b[0;32m---> 16\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#Y_pred, Y_pred_prob = predict(model, val_generator, run)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#run[\"plot\"](history,Y_pred_prob, Y_val)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#evaluate_model(Y_val, Y_pred, Y_pred_prob, history, val_generator, run)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_generator, val_generator, Y_val, model, history\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_generator, val_generator, run)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mearly_stopping\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     20\u001b[0m     callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39mmonitor, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbose\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     loss_hist \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[monitor]\n\u001b[1;32m     25\u001b[0m     run[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetermined_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(loss_hist) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py:1221\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1219\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1221\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1223\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:436\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 436\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:295\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 295\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    298\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:316\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    314\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    319\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:354\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    353\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 354\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    357\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:1032\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1032\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:1104\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1103\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1104\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/tf_utils.py:554\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    552\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t  \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/tf_utils.py:550\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    549\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 550\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    552\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1149\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \n\u001b[1;32m   1128\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1149\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1115\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1114\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test(run, getModelDenseNet121, 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(run, getModelDenseNet201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(run, getModelResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(run, getModelResNet152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(run, getModelMobileNetV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"dataset\"] = \"uncropped_oldLabelling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test(run, getModelDenseNet121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test(run, getModelDenseNet201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test(run, getModelResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test(run, getModelResNet152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test(run, getModelMobileNetV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test(run, getModelResNet152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
